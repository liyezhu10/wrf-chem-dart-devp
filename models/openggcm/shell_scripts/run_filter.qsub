#!/bin/csh
#
# DART software - Copyright UCAR. This open source software is provided
# by UCAR, "as is", without charge, subject to all terms of use at
# http://www.image.ucar.edu/DAReS/DART/DART_download
#
# DART $Id$
#
# Script to assimilate observations using DART and OpenGGCM.
#
#=============================================================================
# This block of directives constitutes the preamble for the LSF queuing system
# LSF is used on the IMAGe Linux cluster 'coral'
# LSF is used on the IBM   'bluefire'
#
# the normal way to submit to the queue is:    bsub < run_filter
#
# an explanation of the most common directives follows:
# -J Job name (master script job.csh presumes filter_server.xxxx.log)
# -o STDOUT filename
# -e STDERR filename
# -P      account
# -q queue    cheapest == [standby, economy, (regular,debug), premium] == $$$$
# -n number of processors  (really)
##=============================================================================
#
#BSUB -J filter
#BSUB -o filter.%J.log
#BSUB -q regular
#BSUB -n 16
#BSUB -R "span[ptile=2]"
#BSUB -P 86850054
#BSUB -W 2:00
#BSUB -N -u ${USER}@ucar.edu
#
##=============================================================================
## This block of directives constitutes the preamble for the PBS queuing system
##
## the normal way to submit to the queue is:    qsub run_filter
##
## an explanation of the most common directives follows:
## -N     Job name
## -r n   Declare job non-rerunable
## -e <arg>  filename for standard error
## -o <arg>  filename for standard out
## -q <arg>   Queue name (small, medium, long, verylong)
## -l nodes=xx:ppn=2   requests BOTH processors on the node. On both bangkok
##                     and calgary, there is no way to 'share' the processors
##                     on the node with another job, so you might as well use
##                     them both. (ppn == Processors Per Node)
##=============================================================================
#
#PBS -N filter
#PBS -e filter.err
#PBS -o filter.log
#PBS -l nodes=1:ppn=32
#PBS -l walltime=0:45:00

#----------------------------------------------------------------------
# Turns out the scripts are a lot more flexible if you don't rely on 
# the queuing-system-specific variables -- so I am converting them to
# 'generic' names and using the generics throughout the remainder.
#----------------------------------------------------------------------

if ($?LSB_QUEUE) then

   #-------------------------------------------------------------------
   # This is used by LSF
   #-------------------------------------------------------------------

   setenv ORIGINALDIR $LS_SUBCWD
   setenv JOBNAME     $LSB_JOBNAME
   setenv JOBID       $LSB_JOBID
   setenv MYQUEUE     $LSB_QUEUE
   setenv MYHOST      $LSB_SUB_HOST
   setenv MPI         mpirun.lsf

else if ($?PBS_QUEUE) then

   #-------------------------------------------------------------------
   # This is used by PBS   todo ... where does xx come from
   #-------------------------------------------------------------------

   setenv ORIGINALDIR $PBS_O_WORKDIR
   setenv JOBNAME     $PBS_JOBNAME
   setenv JOBID       $PBS_JOBID
   setenv MYQUEUE     $PBS_QUEUE
   setenv MYHOST      $PBS_O_HOST
   setenv NUMCPUS     `cat $PBS_NODEFILE | wc -l`
   setenv MPI         "aprun -n $NUMCPUS"

else

   #-------------------------------------------------------------------
   # You can run this interactively to check syntax, file motion, etc.
   #-------------------------------------------------------------------

   setenv ORIGINALDIR `pwd`
   setenv JOBNAME     POP
   setenv JOBID       $$
   setenv MYQUEUE     Interactive
   setenv MYHOST      $HOST
   setenv MPI         csh

endif

#----------------------------------------------------------------------
# Just an echo of the job attributes
#----------------------------------------------------------------------

echo
echo "${JOBNAME} ($JOBID) submitted   from $ORIGINALDIR"
echo "${JOBNAME} ($JOBID) submitted   from $MYHOST"
echo "${JOBNAME} ($JOBID) running in queue $MYQUEUE"
echo "${JOBNAME} ($JOBID) running       on $HOST"
echo "${JOBNAME} ($JOBID) started      at "`date`
echo

#----------------------------------------------------------------------
# actually start doing something ...
#----------------------------------------------------------------------

alias die 'echo FATAL: \!*; exit 1'

cd $ORIGINALDIR

if ( -e   configuration.txt ) then
   source configuration.txt
else
   die "Must have a 'configuration.txt' file in current directory."
endif

cd $RUNPATH

# some systems don't like the -v option to any of the following 

set OSTYPE = `uname -s`
switch ( ${OSTYPE} )
   case IRIX64:
      setenv REMOVE 'rm -rf'
      setenv   COPY 'cp -p'
      setenv   MOVE 'mv -f'
      setenv   LINK 'ln -s'
      breaksw
   case AIX:
      setenv REMOVE 'rm -rf'
      setenv   COPY 'cp -p'
      setenv   MOVE 'mv -f'
      setenv   LINK 'ln -s'
      breaksw
   default:
      setenv REMOVE 'rm -rvf'
      setenv   COPY 'cp -vp'
      setenv   MOVE 'mv -fv'
      setenv   LINK 'ln -sv'
      breaksw
endsw

echo "${JOBNAME} ($JOBID) RUNPATH == $RUNPATH"

#-----------------------------------------------------------------------------
# Set variables containing various directory names where we will GET things
#-----------------------------------------------------------------------------

set DARTROOT = /mnt/lustre/lus0/home/dart/dart/rma_openggcm
set DARTDIR = /mnt/lustre/lus0/home/dart/dart/rma_openggcm/models/openggcm
set OBSERVATIONDIR = $DARTDIR/observations
set BASETIME = 1966-01-01_00:00:00

#-----------------------------------------------------------------------------
# Get the DART executables, scripts, and input files.
# The run-time input (input.nml) must have been staged in $RUNPATH.
# At some point, maybe all these files should be pre-staged. It would 
# enable multiple experiments to be carried out at the same time.
#-----------------------------------------------------------------------------

# executables
${COPY} ${DARTDIR}/work/advance_time.exe .  || exit 1
${COPY} ${DARTDIR}/work/filter.exe       .  || exit 1

set SAMP_ERR_FILE = ${DARTROOT}/assimilation_code/programs/gen_sampling_err_table/work/sampling_error_correction_table.nc
${COPY} ${SAMP_ERR_FILE}  .

# staging inflation files
${COPY} ${DARTDIR}/data/output_priorinf_mean.nc  input_priorinf_mean.nc
${COPY} ${DARTDIR}/data/output_priorinf_sd.nc    input_priorinf_sd.nc

#-----------------------------------------------------------------------------
# Start waiting for something to do ...
#-----------------------------------------------------------------------------

@ ncycle = 0
while ( 1 ) 
   
   find . -name to_dart.semaphore >! semaphore_files.txt
   set nfiles = `cat semaphore_files.txt | wc -l`
   if ($nfiles == $ENSEMBLE_SIZE) then

      #------------------------------------------------------------
      # Get the time 
      #------------------------------------------------------------

      set semaphore = `head -n 1 semaphore_files.txt`
      set TimeString = `head -n 1 $semaphore | sed -e "s/\..*//"`

      echo "DART: Running assimilation for $TimeString at "`date`

      if ( $TimeString == "done" ) exit

      set ModelTime = `echo ${BASETIME} +${TimeString}s | ./advance_time.exe`
      set ModelDDss = `echo ${BASETIME} +${TimeString}s -g | ./advance_time.exe`

      echo "ModelTime is ${ModelTime}"
      echo "ModelDDss is ${ModelDDss}"

      #------------------------------------------------------------
      # link the obs
      #------------------------------------------------------------

      ${REMOVE} obs_seq.out
      ${LINK} ${OBSERVATIONDIR}/obs_seq${ModelTime} obs_seq.out || exit 1

      #------------------------------------------------------------
      # input.nml presumes 'input_list.txt', 'output_list.txt'
      #------------------------------------------------------------

      ls -1 */target/DATA.ionos2.nc >! input_list.txt
      sed -e "s/DATA.ionos2.nc/dart_posterior.nc/" input_list.txt >! output_list.txt

      ${LINK} -s `head -n 1 input_list.txt` .

      ${MPI} ./filter.exe || exit 2

      #------------------------------------------------------------
      # rename output files (inflation, diagnostics, etc) to be unique
      #------------------------------------------------------------
    
      foreach FILE ( preassim_*.nc output_*.nc obs_seq.final )
         set BASE = $FILE:r
         set EXT  = $FILE:e
         \mv -v $FILE ${BASE}.${ModelTime}.${EXT}
      end

      #------------------------------------------------------------
      # Checking for inflation files from the assimilation, use them
      # as input for the next cycle. Since they are only read - we can link.
      #------------------------------------------------------------

      (ls -rt1 output_priorinf_mean.*.nc | tail -n 1 >! latestfile) > & /dev/null
      set nfiles = `cat latestfile | wc -l`
      if ( $nfiles > 0 ) then
         set latest = `cat latestfile`
         ${LINK} $latest input_priorinf_mean.nc
      endif
      
      (ls -rt1 output_priorinf_sd.*.nc | tail -n 1 >! latestfile) > & /dev/null
      set nfiles = `cat latestfile | wc -l`
      if ( $nfiles > 0 ) then
         set latest = `cat latestfile`
         ${LINK} $latest input_priorinf_sd.nc
      endif

      #------------------------------------------------------------
      # signal that DART is done
      # remove the 'to_dart' semaphore
      # create the 'from_dart' semaphore
      #------------------------------------------------------------

      @ ncycle ++  # debug

      echo "600"                          >! temp_file.txt
      echo "Assimilation for $TimeString" >> temp_file.txt

      foreach FILE ( `cat semaphore_files.txt` )
         set NEWFILE = `echo ${FILE} | sed -e "s/to_dart/from_dart/"`
         echo "making ${NEWFILE} to signal openggcm at cycle $ncycle"

         ${REMOVE} ${FILE}
#         ${REMOVE} ${FILE} ${NEWFILE}

         ${COPY} temp_file.txt ${NEWFILE}
         ${COPY} temp_file.txt ${NEWFILE}.safety

         if ( $ncycle > 3 ) echo "DONE"
#        if ( $ncycle > 3 ) echo "DONE" >! ${NEWFILE}
#        if ( $ncycle > 3 ) echo "DONE" >! ${NEWFILE}.safety

      end
      if ( $ncycle > 3 ) then
         echo "${JOBNAME} ($JOBID) finished at "`date`
         exit
      endif

      ${REMOVE} semaphore_files.txt

   endif

   sleep 1

end  # End of 'infinite' wait loop

echo "${JOBNAME} ($JOBID) finished at "`date`

exit 0

# <next few lines under version control, do not edit>
# $URL$
# $Revision$
# $Date$
